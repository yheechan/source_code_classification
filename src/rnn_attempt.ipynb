{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "import data\n",
    "import utils\n",
    "import info_recorder as ir\n",
    "import data_loader as dl\n",
    "import initializer as init\n",
    "import trainer as tn\n",
    "import tester as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, size_info, size_dict = data.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceCode_np = df.sourceCode.values\n",
    "codeClass_np = df.classLabel.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('../model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, build vocabulary, encode tokens\n",
    "print(\"Tokenizing...\\n\")\n",
    "tokenized_sourceCodes, ch2idx, max_len = utils.tokenize(sourceCode_np)\n",
    "input_ids = utils.encode(tokenized_sourceCodes, ch2idx, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(ch2idx.keys())\n",
    "ch_list = list(ch2idx.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_vectors():\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(ch2idx), 300))\n",
    "    embeddings[ch2idx['<pad>']] = np.zeros((300,))\n",
    "\n",
    "    word_list = list(ch2idx.keys())\n",
    "    id_list = list(ch2idx.values())\n",
    "\n",
    "    # Load pretrained vectors\n",
    "    count = 0\n",
    "    for i in range(len(ch2idx)):\n",
    "        word_position = id_list.index(i)\n",
    "        word = word_list[word_position]\n",
    "\n",
    "        if word in word2vec_model:\n",
    "            count += 1\n",
    "            embeddings[ch2idx[word]] = word2vec_model[word]\n",
    "\n",
    "    print(f\"There are {count} / {len(ch2idx)} pretrained vectors found.\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 74 / 97 pretrained vectors found.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained vectors\n",
    "embeddings = load_pretrained_vectors()\n",
    "embeddings = torch.tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97, 300])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir.record_ch2idx(ch2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_class2idx, class2idx, num_classes = utils.tokenize_encode_class(codeClass_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    input_ids, encoded_class2idx, test_size = 0.1, random_state = 43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    train_inputs, train_labels, test_size = 0.1, random_state = 43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, val_dataloader, test_dataloader = dl.data_loader(train_inputs, val_inputs, test_inputs, train_labels, val_labels, test_labels, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "             # Load batch to GPU\n",
    "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            print(b_input_ids.type())\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 124])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "    print(b_input_ids.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('overfitRNN/tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing without pretrained model!!!\n",
      "RNNClassifier(\n",
      "  (emb): Embedding(97, 100)\n",
      "  (rnn): LSTM(100, 100, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc1): Linear(in_features=200, out_features=300, bias=True)\n",
      "  (fc4): Linear(in_features=300, out_features=21, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   2.216269   |  1.451440  |   56.29   |  110.46  \n",
      "   2    |   1.197605   |  0.951771  |   72.31   |  111.21  \n",
      "   3    |   0.923150   |  0.833584  |   75.70   |  110.29  \n",
      "   4    |   0.808864   |  0.772096  |   77.42   |  111.10  \n",
      "   5    |   0.732118   |  0.731341  |   78.65   |  110.83  \n",
      "   6    |   0.672669   |  0.693519  |   79.79   |  111.44  \n",
      "   7    |   0.626605   |  0.680132  |   80.35   |  111.24  \n",
      "   8    |   0.591741   |  0.662210  |   81.07   |  111.15  \n",
      "   9    |   0.561495   |  0.642473  |   81.27   |  111.24  \n",
      "  10    |   0.538854   |  0.626977  |   82.10   |  111.49  \n",
      "  11    |   0.516350   |  0.632977  |   82.00   |  111.90  \n",
      "  12    |   0.503214   |  0.628722  |   82.17   |  111.07  \n",
      "  13    |   0.485304   |  0.613953  |   82.74   |  112.97  \n",
      "  14    |   0.472336   |  0.620057  |   82.46   |  111.55  \n",
      "  15    |   0.458785   |  0.629214  |   82.61   |  112.41  \n",
      "  16    |   0.444310   |  0.614934  |   82.73   |  112.74  \n",
      "  17    |   0.436571   |  0.621407  |   82.89   |  112.30  \n",
      "  18    |   0.426649   |  0.630330  |   83.04   |  112.22  \n",
      "  19    |   0.419954   |  0.611286  |   83.08   |  111.91  \n",
      "  20    |   0.413324   |  0.608537  |   83.06   |  113.19  \n",
      "  21    |   0.404244   |  0.609642  |   83.53   |  112.52  \n",
      "  22    |   0.398285   |  0.618521  |   83.28   |  112.82  \n",
      "  23    |   0.391743   |  0.609831  |   83.44   |  112.89  \n",
      "  24    |   0.387015   |  0.602948  |   83.54   |  111.86  \n",
      "  25    |   0.381375   |  0.626865  |   83.53   |  112.83  \n",
      "  26    |   0.378888   |  0.626165  |   83.63   |  112.30  \n",
      "  27    |   0.371142   |  0.611497  |   83.72   |  111.35  \n",
      "  28    |   0.366054   |  0.624514  |   83.54   |  111.70  \n",
      "  29    |   0.364045   |  0.631637  |   83.57   |  115.45  \n",
      "  30    |   0.359751   |  0.619781  |   83.46   |  113.77  \n",
      "  31    |   0.354119   |  0.621911  |   83.80   |  111.95  \n",
      "  32    |   0.350991   |  0.614662  |   83.92   |  111.47  \n",
      "  33    |   0.348956   |  0.638126  |   83.89   |  111.15  \n",
      "  34    |   0.343211   |  0.625762  |   84.12   |  111.91  \n",
      "  35    |   0.343058   |  0.611219  |   83.95   |  112.65  \n",
      "  36    |   0.340138   |  0.620159  |   84.07   |  113.37  \n",
      "  37    |   0.336792   |  0.626461  |   83.84   |  112.63  \n",
      "  38    |   0.335586   |  0.649417  |   83.75   |  111.50  \n",
      "  39    |   0.331901   |  0.633429  |   84.04   |  112.82  \n",
      "  40    |   0.326770   |  0.634537  |   84.11   |  113.17  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.12%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "tn.set_seed(42)\n",
    "cnn_rand, optimizer = init.initilize_model(pretrained_embedding=None,\n",
    "                                           device=device,\n",
    "                                           vocab_size=len(ch2idx),\n",
    "                                           embed_dim=100,\n",
    "                                           hidden_size=100,\n",
    "                                           num_classes=len(class2idx),\n",
    "                                           n_layers=3,\n",
    "                                           dropout=0.2,\n",
    "                                           learning_rate=0.001,\n",
    "                                           optimizerName=\"Adam\",\n",
    "                                           modelType=\"RNN\")\n",
    "\n",
    "print(cnn_rand)\n",
    "\n",
    "tn.train(device, cnn_rand, optimizer, train_dataloader, 'test29', writer, val_dataloader, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  0.6552621992925803\n",
      "test acc:  83.61904761904762\n"
     ]
    }
   ],
   "source": [
    "tot_pred, tot_label = ts.test(device, cnn_rand, test_dataloader)\n",
    "\n",
    "results = metrics.classification_report(tot_label.cpu(), tot_pred.cpu(), output_dict=True)\n",
    "results_df = pd.DataFrame.from_dict(results).transpose()\n",
    "results_df.to_excel('../result/29_overfitRNN_test29.xlsx', sheet_name='sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing without pretrained model!!!\n",
      "RNNClassifier(\n",
      "  (emb): Embedding(97, 100)\n",
      "  (rnn): LSTM(100, 100, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc1): Linear(in_features=200, out_features=300, bias=True)\n",
      "  (fc4): Linear(in_features=300, out_features=21, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   2.320105   |  1.618197  |   49.52   |  110.14  \n",
      "   2    |   1.452031   |  1.120646  |   66.98   |  111.27  \n",
      "   3    |   1.122011   |  0.956497  |   72.38   |  111.68  \n",
      "   4    |   0.993471   |  0.870230  |   75.03   |  111.09  \n",
      "   5    |   0.915376   |  0.820823  |   76.61   |  111.17  \n",
      "   6    |   0.860823   |  0.784516  |   77.24   |  111.49  \n",
      "   7    |   0.821962   |  0.766448  |   77.84   |  111.41  \n",
      "   8    |   0.788362   |  0.746976  |   78.38   |  111.09  \n",
      "   9    |   0.760905   |  0.726712  |   79.16   |  110.99  \n",
      "  10    |   0.738555   |  0.719459  |   79.58   |  111.58  \n",
      "  11    |   0.719171   |  0.719244  |   79.65   |  111.06  \n",
      "  12    |   0.701044   |  0.697951  |   80.30   |  111.26  \n",
      "  13    |   0.687822   |  0.700118  |   80.23   |  110.85  \n",
      "  14    |   0.672753   |  0.683836  |   80.53   |  111.57  \n",
      "  15    |   0.659960   |  0.693401  |   80.70   |  110.68  \n",
      "  16    |   0.647475   |  0.678366  |   80.87   |  111.33  \n",
      "  17    |   0.637406   |  0.664645  |   81.47   |  111.21  \n",
      "  18    |   0.628231   |  0.665538  |   81.39   |  111.46  \n",
      "  19    |   0.616356   |  0.663712  |   81.66   |  110.86  \n",
      "  20    |   0.611399   |  0.659120  |   81.54   |  111.01  \n",
      "  21    |   0.603978   |  0.660864  |   81.43   |  111.31  \n",
      "  22    |   0.594938   |  0.662072  |   81.57   |  111.24  \n",
      "  23    |   0.586128   |  0.652583  |   81.88   |  110.91  \n",
      "  24    |   0.579548   |  0.652568  |   81.99   |  110.51  \n",
      "  25    |   0.574984   |  0.657791  |   82.06   |  111.36  \n",
      "  26    |   0.569709   |  0.645571  |   82.13   |  112.66  \n",
      "  27    |   0.559970   |  0.641012  |   82.44   |  112.35  \n",
      "  28    |   0.558818   |  0.634545  |   82.41   |  113.50  \n",
      "  29    |   0.555347   |  0.643603  |   82.60   |  116.94  \n",
      "  30    |   0.548156   |  0.652208  |   82.58   |  117.52  \n",
      "  31    |   0.542277   |  0.638801  |   82.46   |  117.39  \n",
      "  32    |   0.540864   |  0.647694  |   82.41   |  118.06  \n",
      "  33    |   0.539279   |  0.660182  |   82.32   |  113.75  \n",
      "  34    |   0.532959   |  0.644295  |   82.44   |  113.04  \n",
      "  35    |   0.531497   |  0.637993  |   82.79   |  111.02  \n",
      "  36    |   0.524447   |  0.654203  |   82.59   |  111.02  \n",
      "  37    |   0.524085   |  0.632178  |   82.74   |  113.69  \n",
      "  38    |   0.521076   |  0.639130  |   82.60   |  113.31  \n",
      "  39    |   0.516012   |  0.633873  |   83.07   |  114.06  \n",
      "  40    |   0.512582   |  0.636487  |   82.91   |  112.74  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 83.07%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "tn.set_seed(42)\n",
    "cnn_rand, optimizer = init.initilize_model(pretrained_embedding=None,\n",
    "                                           device=device,\n",
    "                                           vocab_size=len(ch2idx),\n",
    "                                           embed_dim=100,\n",
    "                                           hidden_size=100,\n",
    "                                           num_classes=len(class2idx),\n",
    "                                           n_layers=3,\n",
    "                                           dropout=0.5,\n",
    "                                           learning_rate=0.001,\n",
    "                                           optimizerName=\"Adam\",\n",
    "                                           modelType=\"RNN\")\n",
    "\n",
    "print(cnn_rand)\n",
    "\n",
    "tn.train(device, cnn_rand, optimizer, train_dataloader, 'test30', writer, val_dataloader, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  0.6403531291832526\n",
      "test acc:  82.4047619047619\n"
     ]
    }
   ],
   "source": [
    "tot_pred, tot_label = ts.test(device, cnn_rand, test_dataloader)\n",
    "\n",
    "results = metrics.classification_report(tot_label.cpu(), tot_pred.cpu(), output_dict=True)\n",
    "results_df = pd.DataFrame.from_dict(results).transpose()\n",
    "results_df.to_excel('../result/30_overfitRNN_test30.xlsx', sheet_name='sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing without pretrained model!!!\n",
      "RNNClassifier(\n",
      "  (emb): Embedding(97, 100)\n",
      "  (rnn): LSTM(100, 100, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (fc1): Linear(in_features=200, out_features=300, bias=True)\n",
      "  (fc4): Linear(in_features=300, out_features=21, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   2.133701   |  1.370714  |   57.70   |  112.75  \n",
      "   2    |   1.193715   |  0.967960  |   71.85   |  112.11  \n",
      "   3    |   0.943489   |  0.848652  |   75.04   |  111.96  \n",
      "   4    |   0.836379   |  0.793319  |   76.96   |  112.33  \n",
      "   5    |   0.767676   |  0.757590  |   77.87   |  114.73  \n",
      "   6    |   0.715471   |  0.710469  |   79.34   |  115.85  \n",
      "   7    |   0.674643   |  0.692945  |   79.60   |  111.99  \n",
      "   8    |   0.642583   |  0.677809  |   80.53   |  110.22  \n",
      "   9    |   0.617396   |  0.668722  |   80.53   |  109.80  \n",
      "  10    |   0.598148   |  0.651355  |   81.01   |  110.14  \n",
      "  11    |   0.576513   |  0.643238  |   81.59   |  110.15  \n",
      "  12    |   0.561260   |  0.631839  |   81.74   |  110.11  \n",
      "  13    |   0.542840   |  0.621935  |   81.90   |  110.03  \n",
      "  14    |   0.531406   |  0.621341  |   82.17   |  110.32  \n",
      "  15    |   0.521329   |  0.632550  |   82.17   |  111.27  \n",
      "  16    |   0.508592   |  0.617530  |   82.42   |  110.53  \n",
      "  17    |   0.499376   |  0.613171  |   82.41   |  110.83  \n",
      "  18    |   0.491719   |  0.612490  |   82.50   |  110.38  \n",
      "  19    |   0.483404   |  0.611614  |   82.69   |  111.11  \n",
      "  20    |   0.476836   |  0.618586  |   82.56   |  111.30  \n",
      "  21    |   0.467966   |  0.602542  |   82.93   |  112.18  \n",
      "  22    |   0.462573   |  0.597712  |   83.30   |  111.93  \n",
      "  23    |   0.457903   |  0.607375  |   83.01   |  111.32  \n",
      "  24    |   0.450263   |  0.597002  |   83.19   |  111.84  \n",
      "  25    |   0.447609   |  0.605543  |   83.27   |  112.68  \n",
      "  26    |   0.442927   |  0.598518  |   83.54   |  115.58  \n",
      "  27    |   0.435004   |  0.599897  |   83.11   |  112.97  \n",
      "  28    |   0.431560   |  0.603620  |   83.46   |  111.21  \n",
      "  29    |   0.428852   |  0.606247  |   83.71   |  113.12  \n",
      "  30    |   0.424952   |  0.598527  |   83.29   |  113.27  \n",
      "  31    |   0.422185   |  0.613415  |   83.17   |  113.09  \n",
      "  32    |   0.416091   |  0.608265  |   83.47   |  107.66  \n",
      "  33    |   0.415373   |  0.624332  |   83.32   |  112.27  \n",
      "  34    |   0.409708   |  0.605934  |   83.19   |  112.39  \n",
      "  35    |   0.405146   |  0.600776  |   84.04   |  110.50  \n",
      "  36    |   0.404866   |  0.605022  |   83.70   |  111.08  \n",
      "  37    |   0.400420   |  0.614015  |   83.51   |  111.04  \n",
      "  38    |   0.397228   |  0.607979  |   83.19   |  116.24  \n",
      "  39    |   0.396366   |  0.609000  |   83.77   |  114.54  \n",
      "  40    |   0.391441   |  0.595897  |   83.81   |  149.82  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.04%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "tn.set_seed(42)\n",
    "cnn_rand, optimizer = init.initilize_model(pretrained_embedding=None,\n",
    "                                           device=device,\n",
    "                                           vocab_size=len(ch2idx),\n",
    "                                           embed_dim=100,\n",
    "                                           hidden_size=100,\n",
    "                                           num_classes=len(class2idx),\n",
    "                                           n_layers=3,\n",
    "                                           dropout=0.3,\n",
    "                                           learning_rate=0.001,\n",
    "                                           optimizerName=\"Adam\",\n",
    "                                           modelType=\"RNN\")\n",
    "\n",
    "print(cnn_rand)\n",
    "\n",
    "tn.train(device, cnn_rand, optimizer, train_dataloader, 'test31', writer, val_dataloader, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  0.6016800244294461\n",
      "test acc:  83.55714285714285\n"
     ]
    }
   ],
   "source": [
    "tot_pred, tot_label = ts.test(device, cnn_rand, test_dataloader)\n",
    "\n",
    "results = metrics.classification_report(tot_label.cpu(), tot_pred.cpu(), output_dict=True)\n",
    "results_df = pd.DataFrame.from_dict(results).transpose()\n",
    "results_df.to_excel('../result/31_overfitRNN_test31.xlsx', sheet_name='sheet1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "348b9cd948ce87438be2e622031b2ecfa29bc2d3ecc0fd03127b9a24b30227df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
