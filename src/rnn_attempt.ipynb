{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "import data\n",
    "import utils\n",
    "import info_recorder as ir\n",
    "import data_loader as dl\n",
    "import initializer as init\n",
    "import trainer as tn\n",
    "import tester as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, size_info, size_dict = data.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceCode_np = df.sourceCode.values\n",
    "codeClass_np = df.classLabel.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('../model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, build vocabulary, encode tokens\n",
    "print(\"Tokenizing...\\n\")\n",
    "tokenized_sourceCodes, ch2idx, max_len = utils.tokenize(sourceCode_np)\n",
    "input_ids = utils.encode(tokenized_sourceCodes, ch2idx, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(ch2idx.keys())\n",
    "ch_list = list(ch2idx.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_vectors():\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(ch2idx), 300))\n",
    "    embeddings[ch2idx['<pad>']] = np.zeros((300,))\n",
    "\n",
    "    word_list = list(ch2idx.keys())\n",
    "    id_list = list(ch2idx.values())\n",
    "\n",
    "    # Load pretrained vectors\n",
    "    count = 0\n",
    "    for i in range(len(ch2idx)):\n",
    "        word_position = id_list.index(i)\n",
    "        word = word_list[word_position]\n",
    "\n",
    "        if word in word2vec_model:\n",
    "            count += 1\n",
    "            embeddings[ch2idx[word]] = word2vec_model[word]\n",
    "\n",
    "    print(f\"There are {count} / {len(ch2idx)} pretrained vectors found.\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 74 / 97 pretrained vectors found.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained vectors\n",
    "embeddings = load_pretrained_vectors()\n",
    "embeddings = torch.tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97, 300])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir.record_ch2idx(ch2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_class2idx, class2idx, num_classes = utils.tokenize_encode_class(codeClass_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    input_ids, encoded_class2idx, test_size = 0.1, random_state = 43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    train_inputs, train_labels, test_size = 0.1, random_state = 43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, val_dataloader, test_dataloader = dl.data_loader(train_inputs, val_inputs, test_inputs, train_labels, val_labels, test_labels, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "             # Load batch to GPU\n",
    "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            print(b_input_ids.type())\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 124])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "    print(b_input_ids.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('overfitRNN/tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing without pretrained model!!!\n",
      "RNNClassifier(\n",
      "  (emb): Embedding(97, 100)\n",
      "  (rnn): LSTM(100, 100, num_layers=3, batch_first=True, bidirectional=True)\n",
      "  (fc1): Linear(in_features=200, out_features=300, bias=True)\n",
      "  (fc4): Linear(in_features=300, out_features=21, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   1.898185   |  1.174756  |   65.07   |  106.55  \n",
      "   2    |   0.975482   |  0.860161  |   75.05   |  106.70  \n",
      "   3    |   0.764490   |  0.751425  |   78.03   |  107.93  \n",
      "   4    |   0.645863   |  0.712622  |   79.27   |  108.22  \n",
      "   5    |   0.561175   |  0.668954  |   80.52   |  107.08  \n",
      "   6    |   0.490347   |  0.676423  |   80.52   |  106.13  \n",
      "   7    |   0.430986   |  0.677771  |   81.03   |  106.35  \n",
      "   8    |   0.385158   |  0.682339  |   81.60   |  105.90  \n",
      "   9    |   0.341640   |  0.695439  |   81.36   |  106.48  \n",
      "  10    |   0.306526   |  0.743779  |   81.20   |  106.21  \n",
      "  11    |   0.277048   |  0.762548  |   81.68   |  106.42  \n",
      "  12    |   0.249790   |  0.780971  |   81.53   |  106.00  \n",
      "  13    |   0.229034   |  0.819209  |   81.54   |  106.41  \n",
      "  14    |   0.212629   |  0.832527  |   81.40   |  106.10  \n",
      "  15    |   0.194889   |  0.841611  |   81.26   |  106.51  \n",
      "  16    |   0.183800   |  0.890100  |   81.21   |  105.72  \n",
      "  17    |   0.169904   |  0.901764  |   81.09   |  106.34  \n",
      "  18    |   0.163649   |  0.916988  |   81.02   |  106.38  \n",
      "  19    |   0.152958   |  0.955998  |   80.75   |  106.34  \n",
      "  20    |   0.144758   |  0.964978  |   81.15   |  106.26  \n",
      "  21    |   0.139186   |  0.978008  |   81.06   |  106.31  \n",
      "  22    |   0.136594   |  1.031842  |   81.12   |  105.77  \n",
      "  23    |   0.128600   |  1.033598  |   81.20   |  106.26  \n",
      "  24    |   0.125857   |  1.052101  |   81.20   |  106.14  \n",
      "  25    |   0.126262   |  1.043337  |   81.60   |  106.29  \n",
      "  26    |   0.120566   |  1.068688  |   81.01   |  106.26  \n",
      "  27    |   0.119985   |  1.065272  |   81.16   |  106.21  \n",
      "  28    |   0.114493   |  1.046834  |   81.41   |  105.90  \n",
      "  29    |   0.116544   |  1.086001  |   81.06   |  106.32  \n",
      "  30    |   0.111154   |  1.116343  |   81.28   |  105.82  \n",
      "  31    |   0.110304   |  1.129415  |   81.40   |  106.22  \n",
      "  32    |   0.109632   |  1.095027  |   81.08   |  105.95  \n",
      "  33    |   0.105324   |  1.145716  |   81.33   |  106.33  \n",
      "  34    |   0.105372   |  1.165247  |   80.98   |  106.38  \n",
      "  35    |   0.103362   |  1.156551  |   81.23   |  106.08  \n",
      "  36    |   0.104433   |  1.150390  |   81.50   |  106.36  \n",
      "  37    |   0.098096   |  1.151931  |   80.86   |  106.45  \n",
      "  38    |   0.102122   |  1.122942  |   81.30   |  106.03  \n",
      "  39    |   0.099674   |  1.202528  |   81.05   |  106.32  \n",
      "  40    |   0.096394   |  1.190077  |   81.06   |  106.13  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 81.68%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "tn.set_seed(42)\n",
    "cnn_rand, optimizer = init.initilize_model(pretrained_embedding=None,\n",
    "                                           device=device,\n",
    "                                           vocab_size=len(ch2idx),\n",
    "                                           embed_dim=100,\n",
    "                                           hidden_size=100,\n",
    "                                           num_classes=len(class2idx),\n",
    "                                           n_layers=3,\n",
    "                                           dropout=0.0,\n",
    "                                           learning_rate=0.001,\n",
    "                                           optimizerName=\"Adam\",\n",
    "                                           modelType=\"RNN\")\n",
    "\n",
    "print(cnn_rand)\n",
    "\n",
    "tn.train(device, cnn_rand, optimizer, train_dataloader, 'test32', writer, val_dataloader, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  1.2159057943593887\n",
      "test acc:  80.82857142857142\n"
     ]
    }
   ],
   "source": [
    "tot_pred, tot_label = ts.test(device, cnn_rand, test_dataloader)\n",
    "\n",
    "results = metrics.classification_report(tot_label.cpu(), tot_pred.cpu(), output_dict=True)\n",
    "results_df = pd.DataFrame.from_dict(results).transpose()\n",
    "results_df.to_excel('../result/32_overfitRNN_test32.xlsx', sheet_name='sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing without pretrained model!!!\n",
      "RNNClassifier(\n",
      "  (emb): Embedding(97, 100)\n",
      "  (rnn): LSTM(100, 100, num_layers=3, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (fc1): Linear(in_features=200, out_features=300, bias=True)\n",
      "  (fc4): Linear(in_features=300, out_features=21, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   2.394310   |  1.833423  |   41.71   |  110.52  \n",
      "   2    |   1.405896   |  1.087618  |   67.84   |  110.01  \n",
      "   3    |   0.970974   |  0.856075  |   75.17   |  111.03  \n",
      "   4    |   0.807076   |  0.787752  |   77.10   |  110.38  \n",
      "   5    |   0.711184   |  0.721605  |   78.64   |  110.38  \n",
      "   6    |   0.639781   |  0.697769  |   79.51   |  109.99  \n",
      "   7    |   0.583452   |  0.673317  |   80.29   |  110.63  \n",
      "   8    |   0.540771   |  0.651374  |   81.27   |  110.71  \n",
      "   9    |   0.501685   |  0.644377  |   81.61   |  110.27  \n",
      "  10    |   0.472009   |  0.647481  |   81.55   |  110.36  \n",
      "  11    |   0.444905   |  0.656209  |   81.75   |  110.50  \n",
      "  12    |   0.424468   |  0.661395  |   82.05   |  111.00  \n",
      "  13    |   0.404981   |  0.641055  |   82.54   |  110.11  \n",
      "  14    |   0.389205   |  0.650397  |   82.15   |  110.64  \n",
      "  15    |   0.373283   |  0.646963  |   82.73   |  110.21  \n",
      "  16    |   0.357856   |  0.653791  |   82.60   |  110.62  \n",
      "  17    |   0.347341   |  0.670638  |   82.24   |  109.87  \n",
      "  18    |   0.336987   |  0.660150  |   82.69   |  110.77  \n",
      "  19    |   0.326916   |  0.669244  |   82.64   |  110.10  \n",
      "  20    |   0.320088   |  0.687497  |   82.67   |  110.37  \n",
      "  21    |   0.308522   |  0.680764  |   82.60   |  109.75  \n",
      "  22    |   0.300990   |  0.689448  |   82.88   |  110.63  \n",
      "  23    |   0.295778   |  0.700325  |   82.39   |  110.09  \n",
      "  24    |   0.290838   |  0.677111  |   82.56   |  110.02  \n",
      "  25    |   0.282859   |  0.706369  |   82.81   |  109.76  \n",
      "  26    |   0.275692   |  0.696590  |   83.03   |  110.33  \n",
      "  27    |   0.271813   |  0.708087  |   83.24   |  110.30  \n",
      "  28    |   0.267259   |  0.697912  |   82.91   |  109.65  \n",
      "  29    |   0.266086   |  0.727640  |   83.13   |  109.93  \n",
      "  30    |   0.260407   |  0.715304  |   82.89   |  109.99  \n",
      "  31    |   0.257031   |  0.718750  |   83.13   |  110.46  \n",
      "  32    |   0.254267   |  0.722516  |   82.85   |  109.65  \n",
      "  33    |   0.247945   |  0.722625  |   82.60   |  110.23  \n",
      "  34    |   0.246618   |  0.722914  |   83.04   |  109.80  \n",
      "  35    |   0.242175   |  0.728936  |   82.98   |  110.42  \n",
      "  36    |   0.239243   |  0.755354  |   83.12   |  109.64  \n",
      "  37    |   0.238759   |  0.740130  |   83.19   |  110.60  \n",
      "  38    |   0.233633   |  0.726664  |   83.32   |  110.08  \n",
      "  39    |   0.235139   |  0.740924  |   82.79   |  110.25  \n",
      "  40    |   0.227350   |  0.729156  |   83.15   |  109.82  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 83.32%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "tn.set_seed(42)\n",
    "cnn_rand, optimizer = init.initilize_model(pretrained_embedding=None,\n",
    "                                           device=device,\n",
    "                                           vocab_size=len(ch2idx),\n",
    "                                           embed_dim=100,\n",
    "                                           hidden_size=100,\n",
    "                                           num_classes=len(class2idx),\n",
    "                                           n_layers=3,\n",
    "                                           dropout=0.1,\n",
    "                                           learning_rate=0.001,\n",
    "                                           optimizerName=\"Adam\",\n",
    "                                           modelType=\"RNN\")\n",
    "\n",
    "print(cnn_rand)\n",
    "\n",
    "tn.train(device, cnn_rand, optimizer, train_dataloader, 'test33', writer, val_dataloader, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  0.7444851442107132\n",
      "test acc:  83.25238095238096\n"
     ]
    }
   ],
   "source": [
    "tot_pred, tot_label = ts.test(device, cnn_rand, test_dataloader)\n",
    "\n",
    "results = metrics.classification_report(tot_label.cpu(), tot_pred.cpu(), output_dict=True)\n",
    "results_df = pd.DataFrame.from_dict(results).transpose()\n",
    "results_df.to_excel('../result/33_overfitRNN_test33.xlsx', sheet_name='sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing without pretrained model!!!\n",
      "RNNClassifier(\n",
      "  (emb): Embedding(97, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc1): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (fc4): Linear(in_features=300, out_features=21, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   1.940018   |  1.130193  |   66.66   |  297.49  \n",
      "   2    |   1.006795   |  0.856133  |   75.11   |  296.69  \n",
      "   3    |   0.818541   |  0.783719  |   77.38   |  298.04  \n",
      "   4    |   0.717860   |  0.723074  |   79.04   |  296.47  \n",
      "   5    |   0.648208   |  0.694091  |   79.98   |  296.92  \n",
      "   6    |   0.596966   |  0.656593  |   81.42   |  297.68  \n",
      "   7    |   0.552798   |  0.635621  |   81.82   |  296.58  \n",
      "   8    |   0.518125   |  0.636903  |   82.09   |  296.85  \n",
      "   9    |   0.490046   |  0.615496  |   82.71   |  297.29  \n",
      "  10    |   0.464600   |  0.617228  |   82.75   |  297.02  \n",
      "  11    |   0.447751   |  0.616586  |   83.02   |  297.07  \n",
      "  12    |   0.431023   |  0.609159  |   83.22   |  296.90  \n",
      "  13    |   0.413845   |  0.597111  |   83.28   |  298.58  \n",
      "  14    |   0.400985   |  0.600789  |   83.44   |  297.34  \n",
      "  15    |   0.390366   |  0.609204  |   83.49   |  298.23  \n",
      "  16    |   0.377694   |  0.611048  |   83.89   |  297.37  \n",
      "  17    |   0.370843   |  0.613722  |   83.67   |  298.68  \n",
      "  18    |   0.362566   |  0.610065  |   84.00   |  298.33  \n",
      "  19    |   0.353590   |  0.605133  |   84.03   |  298.31  \n",
      "  20    |   0.349743   |  0.596034  |   84.06   |  298.61  \n",
      "  21    |   0.344973   |  0.624474  |   84.05   |  298.76  \n",
      "  22    |   0.333841   |  0.624204  |   83.82   |  298.56  \n",
      "  23    |   0.330635   |  0.629251  |   83.96   |  298.18  \n",
      "  24    |   0.325996   |  0.611308  |   84.01   |  298.58  \n",
      "  25    |   0.321653   |  0.644360  |   83.90   |  298.72  \n",
      "  26    |   0.319103   |  0.640466  |   84.01   |  297.83  \n",
      "  27    |   0.318739   |  0.632349  |   84.32   |  297.91  \n",
      "  28    |   0.310416   |  0.649612  |   83.92   |  297.89  \n",
      "  29    |   0.308398   |  0.631603  |   83.75   |  298.35  \n",
      "  30    |   0.309947   |  0.621852  |   84.27   |  298.37  \n",
      "  31    |   0.304200   |  0.628317  |   84.12   |  299.20  \n",
      "  32    |   0.302467   |  0.638205  |   84.08   |  298.92  \n",
      "  33    |   0.298758   |  0.624849  |   84.26   |  298.36  \n",
      "  34    |   0.297177   |  0.638504  |   84.29   |  298.95  \n",
      "  35    |   0.293244   |  0.637708  |   83.89   |  297.33  \n",
      "  36    |   0.291508   |  0.622540  |   84.06   |  297.92  \n",
      "  37    |   0.290368   |  0.644640  |   84.08   |  296.88  \n",
      "  38    |   0.287110   |  0.640630  |   84.48   |  297.96  \n",
      "  39    |   0.283519   |  0.631961  |   84.37   |  297.22  \n",
      "  40    |   0.285276   |  0.656550  |   84.39   |  297.34  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.48%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "tn.set_seed(42)\n",
    "cnn_rand, optimizer = init.initilize_model(pretrained_embedding=None,\n",
    "                                           device=device,\n",
    "                                           vocab_size=len(ch2idx),\n",
    "                                           embed_dim=200,\n",
    "                                           hidden_size=200,\n",
    "                                           num_classes=len(class2idx),\n",
    "                                           n_layers=4,\n",
    "                                           dropout=0.2,\n",
    "                                           learning_rate=0.001,\n",
    "                                           optimizerName=\"Adam\",\n",
    "                                           modelType=\"RNN\")\n",
    "\n",
    "print(cnn_rand)\n",
    "\n",
    "tn.train(device, cnn_rand, optimizer, train_dataloader, 'test34', writer, val_dataloader, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  0.6726503895506972\n",
      "test acc:  83.85714285714285\n"
     ]
    }
   ],
   "source": [
    "tot_pred, tot_label = ts.test(device, cnn_rand, test_dataloader)\n",
    "\n",
    "results = metrics.classification_report(tot_label.cpu(), tot_pred.cpu(), output_dict=True)\n",
    "results_df = pd.DataFrame.from_dict(results).transpose()\n",
    "results_df.to_excel('../result/34_overfitRNN_test34.xlsx', sheet_name='sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing without pretrained model!!!\n",
      "RNNClassifier(\n",
      "  (emb): Embedding(97, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc1): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (fc4): Linear(in_features=300, out_features=21, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   1.755507   |  1.034776  |   69.71   |  218.13  \n",
      "   2    |   0.941308   |  0.836113  |   75.71   |  217.94  \n",
      "   3    |   0.782569   |  0.743861  |   78.59   |  219.43  \n",
      "   4    |   0.692912   |  0.695817  |   79.94   |  218.58  \n",
      "   5    |   0.630653   |  0.667529  |   80.86   |  218.87  \n",
      "   6    |   0.582690   |  0.653435  |   80.98   |  218.73  \n",
      "   7    |   0.547676   |  0.638264  |   81.59   |  218.87  \n",
      "   8    |   0.515845   |  0.623400  |   82.23   |  219.36  \n",
      "   9    |   0.489851   |  0.637845  |   82.02   |  218.84  \n",
      "  10    |   0.470591   |  0.632214  |   82.21   |  218.99  \n",
      "  11    |   0.449546   |  0.619330  |   82.73   |  219.59  \n",
      "  12    |   0.436039   |  0.619548  |   82.96   |  219.04  \n",
      "  13    |   0.422632   |  0.628948  |   82.61   |  219.42  \n",
      "  14    |   0.411126   |  0.623688  |   82.72   |  218.44  \n",
      "  15    |   0.396824   |  0.620682  |   83.16   |  219.55  \n",
      "  16    |   0.391938   |  0.632463  |   82.81   |  218.58  \n",
      "  17    |   0.383669   |  0.622208  |   83.44   |  218.71  \n",
      "  18    |   0.373867   |  0.621648  |   83.25   |  217.49  \n",
      "  19    |   0.369101   |  0.623664  |   83.39   |  217.87  \n",
      "  20    |   0.366678   |  0.626876  |   83.23   |  217.79  \n",
      "  21    |   0.355924   |  0.623465  |   83.29   |  217.91  \n",
      "  22    |   0.355084   |  0.629624  |   83.56   |  217.95  \n",
      "  23    |   0.349113   |  0.635239  |   83.50   |  217.37  \n",
      "  24    |   0.346178   |  0.617443  |   83.84   |  217.91  \n",
      "  25    |   0.341670   |  0.635464  |   83.20   |  217.91  \n",
      "  26    |   0.335790   |  0.641991  |   83.70   |  218.34  \n",
      "  27    |   0.333471   |  0.627128  |   83.69   |  218.28  \n",
      "  28    |   0.330837   |  0.629928  |   83.75   |  218.41  \n",
      "  29    |   0.327414   |  0.632790  |   83.49   |  218.44  \n",
      "  30    |   0.324393   |  0.642918  |   83.53   |  218.67  \n",
      "  31    |   0.324559   |  0.642222  |   83.36   |  219.16  \n",
      "  32    |   0.320497   |  0.631539  |   83.75   |  218.41  \n",
      "  33    |   0.316934   |  0.642304  |   83.80   |  219.16  \n",
      "  34    |   0.314964   |  0.655137  |   83.40   |  218.38  \n",
      "  35    |   0.317547   |  0.632795  |   83.50   |  218.77  \n",
      "  36    |   0.313498   |  0.638923  |   83.70   |  218.73  \n",
      "  37    |   0.314257   |  0.637900  |   83.67   |  218.18  \n",
      "  38    |   0.311790   |  0.635183  |   83.43   |  217.31  \n",
      "  39    |   0.309817   |  0.636246  |   83.84   |  218.08  \n",
      "  40    |   0.308359   |  0.659369  |   83.54   |  218.01  \n",
      "  41    |   0.308008   |  0.657399  |   83.74   |  217.09  \n",
      "  42    |   0.306918   |  0.645764  |   83.58   |  217.49  \n",
      "  43    |   0.307673   |  0.649808  |   84.07   |  217.99  \n",
      "  44    |   0.305004   |  0.651464  |   83.90   |  217.77  \n",
      "  45    |   0.303478   |  0.645781  |   83.54   |  218.68  \n",
      "  46    |   0.303809   |  0.648607  |   83.78   |  217.95  \n",
      "  47    |   0.300973   |  0.653449  |   83.70   |  218.64  \n",
      "  48    |   0.296925   |  0.648523  |   83.70   |  218.50  \n",
      "  49    |   0.302535   |  0.653395  |   83.82   |  219.24  \n",
      "  50    |   0.300492   |  0.648330  |   83.92   |  218.52  \n",
      "  51    |   0.300135   |  0.648791  |   83.65   |  219.04  \n",
      "  52    |   0.298820   |  0.647623  |   83.71   |  218.58  \n",
      "  53    |   0.297987   |  0.667317  |   83.46   |  218.73  \n",
      "  54    |   0.300268   |  0.653499  |   83.67   |  218.50  \n",
      "  55    |   0.299080   |  0.661073  |   83.49   |  219.77  \n",
      "  56    |   0.296707   |  0.663118  |   83.98   |  217.41  \n",
      "  57    |   0.296371   |  0.648094  |   84.10   |  218.05  \n",
      "  58    |   0.296771   |  0.636662  |   83.85   |  217.21  \n",
      "  59    |   0.294336   |  0.648509  |   83.99   |  218.11  \n",
      "  60    |   0.294865   |  0.653720  |   83.41   |  217.85  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 84.10%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "tn.set_seed(42)\n",
    "cnn_rand, optimizer = init.initilize_model(pretrained_embedding=None,\n",
    "                                           device=device,\n",
    "                                           vocab_size=len(ch2idx),\n",
    "                                           embed_dim=200,\n",
    "                                           hidden_size=200,\n",
    "                                           num_classes=len(class2idx),\n",
    "                                           n_layers=3,\n",
    "                                           dropout=0.2,\n",
    "                                           learning_rate=0.001,\n",
    "                                           optimizerName=\"Adam\",\n",
    "                                           modelType=\"RNN\")\n",
    "\n",
    "print(cnn_rand)\n",
    "\n",
    "tn.train(device, cnn_rand, optimizer, train_dataloader, 'test35', writer, val_dataloader, epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  0.6712065548414275\n",
      "test acc:  83.25238095238096\n"
     ]
    }
   ],
   "source": [
    "tot_pred, tot_label = ts.test(device, cnn_rand, test_dataloader)\n",
    "\n",
    "results = metrics.classification_report(tot_label.cpu(), tot_pred.cpu(), output_dict=True)\n",
    "results_df = pd.DataFrame.from_dict(results).transpose()\n",
    "results_df.to_excel('../result/35_overfitRNN_test35.xlsx', sheet_name='sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing without pretrained model!!!\n",
      "RNNClassifier(\n",
      "  (emb): Embedding(97, 100)\n",
      "  (rnn): LSTM(100, 100, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc1): Linear(in_features=200, out_features=300, bias=True)\n",
      "  (fc4): Linear(in_features=300, out_features=21, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   2.178552   |  1.475914  |   54.58   |  110.62  \n",
      "   2    |   1.215547   |  0.996089  |   70.68   |  110.37  \n",
      "   3    |   0.930331   |  0.850798  |   74.87   |  110.81  \n",
      "   4    |   0.807368   |  0.761877  |   77.71   |  110.44  \n",
      "   5    |   0.730663   |  0.726634  |   78.86   |  110.27  \n",
      "   6    |   0.672792   |  0.691436  |   79.78   |  110.71  \n",
      "   7    |   0.631053   |  0.676140  |   80.41   |  110.52  \n",
      "   8    |   0.593968   |  0.686432  |   80.15   |  110.60  \n",
      "   9    |   0.565463   |  0.642568  |   81.37   |  110.23  \n",
      "  10    |   0.537996   |  0.629408  |   81.74   |  110.96  \n",
      "  11    |   0.522109   |  0.645273  |   82.02   |  110.35  \n",
      "  12    |   0.502192   |  0.628806  |   82.11   |  110.72  \n",
      "  13    |   0.484192   |  0.630468  |   82.10   |  110.41  \n",
      "  14    |   0.472219   |  0.643086  |   82.15   |  111.25  \n",
      "  15    |   0.457612   |  0.625522  |   82.47   |  110.38  \n",
      "  16    |   0.445982   |  0.629339  |   82.53   |  110.85  \n",
      "  17    |   0.437360   |  0.613907  |   82.52   |  110.72  \n",
      "  18    |   0.429688   |  0.626163  |   82.39   |  111.19  \n",
      "  19    |   0.420451   |  0.629139  |   82.78   |  110.80  \n",
      "  20    |   0.416137   |  0.618695  |   82.84   |  110.77  \n",
      "  21    |   0.407266   |  0.644324  |   82.84   |  111.17  \n",
      "  22    |   0.400680   |  0.619351  |   83.06   |  111.07  \n",
      "  23    |   0.397034   |  0.620963  |   83.03   |  111.25  \n",
      "  24    |   0.388458   |  0.614715  |   83.37   |  110.65  \n",
      "  25    |   0.384857   |  0.623567  |   83.23   |  111.46  \n",
      "  26    |   0.379941   |  0.621137  |   83.08   |  110.79  \n",
      "  27    |   0.373016   |  0.633544  |   83.35   |  111.19  \n",
      "  28    |   0.369860   |  0.626326  |   83.19   |  110.79  \n",
      "  29    |   0.370784   |  0.661036  |   83.08   |  111.79  \n",
      "  30    |   0.366268   |  0.627190  |   83.58   |  110.66  \n",
      "  31    |   0.355635   |  0.636584  |   83.47   |  110.99  \n",
      "  32    |   0.355381   |  0.631408  |   83.43   |  110.87  \n",
      "  33    |   0.353021   |  0.639679  |   83.46   |  111.21  \n",
      "  34    |   0.349159   |  0.642809  |   83.71   |  110.91  \n",
      "  35    |   0.344680   |  0.622289  |   83.77   |  110.77  \n",
      "  36    |   0.344902   |  0.623757  |   83.81   |  111.29  \n",
      "  37    |   0.337110   |  0.645137  |   83.69   |  111.06  \n",
      "  38    |   0.334054   |  0.644398  |   83.13   |  111.08  \n",
      "  39    |   0.333132   |  0.647018  |   83.52   |  110.77  \n",
      "  40    |   0.329685   |  0.642671  |   83.62   |  111.28  \n",
      "  41    |   0.328877   |  0.648541  |   83.41   |  110.81  \n",
      "  42    |   0.324895   |  0.629991  |   83.71   |  111.17  \n",
      "  43    |   0.321408   |  0.660089  |   83.62   |  110.82  \n",
      "  44    |   0.320001   |  0.648564  |   83.58   |  111.44  \n",
      "  45    |   0.318026   |  0.651634  |   83.78   |  110.69  \n",
      "  46    |   0.316756   |  0.634271  |   83.75   |  111.10  \n",
      "  47    |   0.315515   |  0.643912  |   83.90   |  110.88  \n",
      "  48    |   0.312124   |  0.669152  |   83.57   |  111.27  \n",
      "  49    |   0.310394   |  0.649580  |   83.75   |  110.89  \n",
      "  50    |   0.310696   |  0.656815  |   83.78   |  110.81  \n",
      "  51    |   0.305996   |  0.647360  |   83.68   |  111.19  \n",
      "  52    |   0.305342   |  0.649458  |   83.61   |  110.97  \n",
      "  53    |   0.302986   |  0.665915  |   83.78   |  111.20  \n",
      "  54    |   0.303562   |  0.667948  |   83.33   |  110.70  \n",
      "  55    |   0.301036   |  0.670601  |   83.88   |  111.50  \n",
      "  56    |   0.295577   |  0.668899  |   83.71   |  110.87  \n",
      "  57    |   0.299861   |  0.644119  |   83.63   |  111.21  \n",
      "  58    |   0.295871   |  0.674043  |   83.63   |  110.82  \n",
      "  59    |   0.293652   |  0.663888  |   83.78   |  111.77  \n",
      "  60    |   0.295489   |  0.676235  |   83.62   |  110.66  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 83.90%.\n"
     ]
    }
   ],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "tn.set_seed(42)\n",
    "cnn_rand, optimizer = init.initilize_model(pretrained_embedding=None,\n",
    "                                           device=device,\n",
    "                                           vocab_size=len(ch2idx),\n",
    "                                           embed_dim=100,\n",
    "                                           hidden_size=100,\n",
    "                                           num_classes=len(class2idx),\n",
    "                                           n_layers=3,\n",
    "                                           dropout=0.2,\n",
    "                                           learning_rate=0.001,\n",
    "                                           optimizerName=\"Adam\",\n",
    "                                           modelType=\"RNN\")\n",
    "\n",
    "print(cnn_rand)\n",
    "\n",
    "tn.train(device, cnn_rand, optimizer, train_dataloader, 'test36', writer, val_dataloader, epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  0.6682467930373691\n",
      "test acc:  83.65714285714286\n"
     ]
    }
   ],
   "source": [
    "tot_pred, tot_label = ts.test(device, cnn_rand, test_dataloader)\n",
    "\n",
    "results = metrics.classification_report(tot_label.cpu(), tot_pred.cpu(), output_dict=True)\n",
    "results_df = pd.DataFrame.from_dict(results).transpose()\n",
    "results_df.to_excel('../result/36_overfitRNN_test36.xlsx', sheet_name='sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "348b9cd948ce87438be2e622031b2ecfa29bc2d3ecc0fd03127b9a24b30227df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
